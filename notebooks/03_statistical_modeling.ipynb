{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Modeling: Claim Severity and Premium Prediction\n",
        "\n",
        "This notebook builds predictive models for:\n",
        "1. **Claim Severity Prediction**: Predict TotalClaims for policies with claims > 0\n",
        "2. **Premium Optimization**: Predict optimal premium values\n",
        "\n",
        "## Models to Implement\n",
        "- Linear Regression\n",
        "- Decision Trees\n",
        "- Random Forests\n",
        "- XGBoost\n",
        "\n",
        "## Evaluation Metrics\n",
        "- RMSE (Root Mean Squared Error)\n",
        "- R² (Coefficient of Determination)\n",
        "- MAE (Mean Absolute Error)\n",
        "- MAPE (Mean Absolute Percentage Error)\n",
        "\n",
        "## Model Interpretability\n",
        "- Feature Importance Analysis\n",
        "- SHAP (SHapley Additive exPlanations)\n",
        "- LIME (Local Interpretable Model-agnostic Explanations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "\n",
        "from src.data.load_data import load_insurance_data\n",
        "from src.utils.config import REPORTS_DIR, MODELS_DIR\n",
        "from src.modeling.data_preparation import (\n",
        "    prepare_claim_severity_data,\n",
        "    prepare_premium_prediction_data\n",
        ")\n",
        "from src.modeling.models import (\n",
        "    train_linear_regression,\n",
        "    train_decision_tree,\n",
        "    train_random_forest,\n",
        "    train_xgboost,\n",
        "    evaluate_model,\n",
        "    compare_models\n",
        ")\n",
        "from src.modeling.interpretability import (\n",
        "    get_feature_importance,\n",
        "    plot_feature_importance,\n",
        "    explain_with_shap,\n",
        "    plot_shap_summary\n",
        ")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n",
        "\n",
        "# Create directories\n",
        "FIGURES_DIR = REPORTS_DIR / \"figures\"\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = load_insurance_data()\n",
        "print(f\"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
        "print(f\"\\nPolicies with claims: {len(df[df['TotalClaims'] > 0]):,}\")\n",
        "print(f\"Policies without claims: {len(df[df['TotalClaims'] == 0]):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: Claim Severity Prediction\n",
        "\n",
        "Predict TotalClaims for policies that have claims > 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for claim severity prediction\n",
        "X_train_sev, X_test_sev, y_train_sev, y_test_sev, feature_names_sev, preprocessor_sev = \\\n",
        "    prepare_claim_severity_data(df, target_col='TotalClaims', test_size=0.2)\n",
        "\n",
        "print(f\"Training set: {X_train_sev.shape[0]:,} samples, {X_train_sev.shape[1]} features\")\n",
        "print(f\"Test set: {X_test_sev.shape[0]:,} samples\")\n",
        "print(f\"\\nTarget (TotalClaims) statistics:\")\n",
        "print(f\"  Mean: {y_train_sev.mean():.2f} ZAR\")\n",
        "print(f\"  Median: {y_train_sev.median():.2f} ZAR\")\n",
        "print(f\"  Std: {y_train_sev.std():.2f} ZAR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models for claim severity prediction\n",
        "models_sev = {}\n",
        "\n",
        "print(\"Training models for Claim Severity Prediction...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Linear Regression\n",
        "print(\"\\n1. Training Linear Regression...\")\n",
        "model_lr, train_metrics_lr = train_linear_regression(X_train_sev, y_train_sev)\n",
        "models_sev['Linear Regression'] = (model_lr, train_metrics_lr)\n",
        "test_metrics_lr = evaluate_model(model_lr, X_test_sev, y_test_sev)\n",
        "print(f\"   Test RMSE: {test_metrics_lr['rmse']:.2f}, Test R²: {test_metrics_lr['r2']:.4f}\")\n",
        "\n",
        "# Decision Tree\n",
        "print(\"\\n2. Training Decision Tree...\")\n",
        "model_dt, train_metrics_dt = train_decision_tree(X_train_sev, y_train_sev, max_depth=10)\n",
        "models_sev['Decision Tree'] = (model_dt, train_metrics_dt)\n",
        "test_metrics_dt = evaluate_model(model_dt, X_test_sev, y_test_sev)\n",
        "print(f\"   Test RMSE: {test_metrics_dt['rmse']:.2f}, Test R²: {test_metrics_dt['r2']:.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\n3. Training Random Forest...\")\n",
        "model_rf, train_metrics_rf = train_random_forest(X_train_sev, y_train_sev, n_estimators=100)\n",
        "models_sev['Random Forest'] = (model_rf, train_metrics_rf)\n",
        "test_metrics_rf = evaluate_model(model_rf, X_test_sev, y_test_sev)\n",
        "print(f\"   Test RMSE: {test_metrics_rf['rmse']:.2f}, Test R²: {test_metrics_rf['r2']:.4f}\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\n4. Training XGBoost...\")\n",
        "model_xgb, train_metrics_xgb = train_xgboost(X_train_sev, y_train_sev, n_estimators=100)\n",
        "models_sev['XGBoost'] = (model_xgb, train_metrics_xgb)\n",
        "test_metrics_xgb = evaluate_model(model_xgb, X_test_sev, y_test_sev)\n",
        "print(f\"   Test RMSE: {test_metrics_xgb['rmse']:.2f}, Test R²: {test_metrics_xgb['r2']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "comparison_sev = compare_models(models_sev, X_test_sev, y_test_sev)\n",
        "print(\"\\nModel Comparison - Claim Severity Prediction:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_sev.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model_sev = comparison_sev.loc[comparison_sev['Test_R2'].idxmax(), 'Model']\n",
        "print(f\"\\nBest Model (by R²): {best_model_sev}\")\n",
        "print(f\"  Test R²: {comparison_sev.loc[comparison_sev['Test_R2'].idxmax(), 'Test_R2']:.4f}\")\n",
        "print(f\"  Test RMSE: {comparison_sev.loc[comparison_sev['Test_R2'].idxmax(), 'Test_RMSE']:.2f} ZAR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: Premium Prediction\n",
        "\n",
        "Predict optimal premium values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for premium prediction\n",
        "X_train_prem, X_test_prem, y_train_prem, y_test_prem, feature_names_prem, preprocessor_prem = \\\n",
        "    prepare_premium_prediction_data(df, target_col='TotalPremium', test_size=0.2)\n",
        "\n",
        "print(f\"Training set: {X_train_prem.shape[0]:,} samples, {X_train_prem.shape[1]} features\")\n",
        "print(f\"Test set: {X_test_prem.shape[0]:,} samples\")\n",
        "print(f\"\\nTarget (TotalPremium) statistics:\")\n",
        "print(f\"  Mean: {y_train_prem.mean():.2f} ZAR\")\n",
        "print(f\"  Median: {y_train_prem.median():.2f} ZAR\")\n",
        "print(f\"  Std: {y_train_prem.std():.2f} ZAR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models for premium prediction\n",
        "models_prem = {}\n",
        "\n",
        "print(\"Training models for Premium Prediction...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Linear Regression\n",
        "print(\"\\n1. Training Linear Regression...\")\n",
        "model_lr_prem, train_metrics_lr_prem = train_linear_regression(X_train_prem, y_train_prem)\n",
        "models_prem['Linear Regression'] = (model_lr_prem, train_metrics_lr_prem)\n",
        "test_metrics_lr_prem = evaluate_model(model_lr_prem, X_test_prem, y_test_prem)\n",
        "print(f\"   Test RMSE: {test_metrics_lr_prem['rmse']:.2f}, Test R²: {test_metrics_lr_prem['r2']:.4f}\")\n",
        "\n",
        "# Decision Tree\n",
        "print(\"\\n2. Training Decision Tree...\")\n",
        "model_dt_prem, train_metrics_dt_prem = train_decision_tree(X_train_prem, y_train_prem, max_depth=10)\n",
        "models_prem['Decision Tree'] = (model_dt_prem, train_metrics_dt_prem)\n",
        "test_metrics_dt_prem = evaluate_model(model_dt_prem, X_test_prem, y_test_prem)\n",
        "print(f\"   Test RMSE: {test_metrics_dt_prem['rmse']:.2f}, Test R²: {test_metrics_dt_prem['r2']:.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\n3. Training Random Forest...\")\n",
        "model_rf_prem, train_metrics_rf_prem = train_random_forest(X_train_prem, y_train_prem, n_estimators=100)\n",
        "models_prem['Random Forest'] = (model_rf_prem, train_metrics_rf_prem)\n",
        "test_metrics_rf_prem = evaluate_model(model_rf_prem, X_test_prem, y_test_prem)\n",
        "print(f\"   Test RMSE: {test_metrics_rf_prem['rmse']:.2f}, Test R²: {test_metrics_rf_prem['r2']:.4f}\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\n4. Training XGBoost...\")\n",
        "model_xgb_prem, train_metrics_xgb_prem = train_xgboost(X_train_prem, y_train_prem, n_estimators=100)\n",
        "models_prem['XGBoost'] = (model_xgb_prem, train_metrics_xgb_prem)\n",
        "test_metrics_xgb_prem = evaluate_model(model_xgb_prem, X_test_prem, y_test_prem)\n",
        "print(f\"   Test RMSE: {test_metrics_xgb_prem['rmse']:.2f}, Test R²: {test_metrics_xgb_prem['r2']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models for premium prediction\n",
        "comparison_prem = compare_models(models_prem, X_test_prem, y_test_prem)\n",
        "print(\"\\nModel Comparison - Premium Prediction:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_prem.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model_prem = comparison_prem.loc[comparison_prem['Test_R2'].idxmax(), 'Model']\n",
        "print(f\"\\nBest Model (by R²): {best_model_prem}\")\n",
        "print(f\"  Test R²: {comparison_prem.loc[comparison_prem['Test_R2'].idxmax(), 'Test_R2']:.4f}\")\n",
        "print(f\"  Test RMSE: {comparison_prem.loc[comparison_prem['Test_R2'].idxmax(), 'Test_RMSE']:.2f} ZAR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Interpretability: Feature Importance Analysis\n",
        "\n",
        "Analyze which features are most influential in predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance for best claim severity model\n",
        "best_sev_model_name = best_model_sev\n",
        "best_sev_model = models_sev[best_sev_model_name][0]\n",
        "\n",
        "importance_sev = get_feature_importance(best_sev_model, feature_names_sev)\n",
        "print(f\"\\nTop 10 Features for {best_sev_model_name} (Claim Severity):\")\n",
        "print(\"=\"*80)\n",
        "print(importance_sev.head(10).to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "plot_feature_importance(\n",
        "    importance_sev,\n",
        "    top_n=10,\n",
        "    title=f\"Top 10 Feature Importance - {best_sev_model_name} (Claim Severity)\",\n",
        "    save_path=FIGURES_DIR / '10_feature_importance_claim_severity.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance for best premium prediction model\n",
        "best_prem_model_name = best_model_prem\n",
        "best_prem_model = models_prem[best_prem_model_name][0]\n",
        "\n",
        "importance_prem = get_feature_importance(best_prem_model, feature_names_prem)\n",
        "print(f\"\\nTop 10 Features for {best_prem_model_name} (Premium Prediction):\")\n",
        "print(\"=\"*80)\n",
        "print(importance_prem.head(10).to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "plot_feature_importance(\n",
        "    importance_prem,\n",
        "    top_n=10,\n",
        "    title=f\"Top 10 Feature Importance - {best_prem_model_name} (Premium Prediction)\",\n",
        "    save_path=FIGURES_DIR / '11_feature_importance_premium.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP Analysis (for Best Model)\n",
        "\n",
        "Use SHAP to understand how individual features influence predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP analysis for best claim severity model\n",
        "print(f\"Generating SHAP explanations for {best_sev_model_name} (Claim Severity)...\")\n",
        "shap_result_sev = explain_with_shap(\n",
        "    best_sev_model,\n",
        "    X_test_sev[:100],  # Use sample for faster computation\n",
        "    feature_names_sev,\n",
        "    max_evals=100\n",
        ")\n",
        "\n",
        "if shap_result_sev:\n",
        "    plot_shap_summary(\n",
        "        shap_result_sev,\n",
        "        save_path=FIGURES_DIR / '12_shap_summary_claim_severity.png'\n",
        "    )\n",
        "    print(\"\\nSHAP analysis completed!\")\n",
        "else:\n",
        "    print(\"SHAP analysis not available. Install SHAP: pip install shap\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Business Recommendations\n",
        "\n",
        "Based on model performance and feature importance analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. CLAIM SEVERITY PREDICTION:\")\n",
        "print(f\"   Best Model: {best_sev_model_name}\")\n",
        "print(f\"   Test R²: {comparison_sev.loc[comparison_sev['Model'] == best_sev_model_name, 'Test_R2'].values[0]:.4f}\")\n",
        "print(f\"   Test RMSE: {comparison_sev.loc[comparison_sev['Model'] == best_sev_model_name, 'Test_RMSE'].values[0]:.2f} ZAR\")\n",
        "print(f\"\\n   Top 5 Features:\")\n",
        "for idx, row in importance_sev.head(5).iterrows():\n",
        "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n2. PREMIUM PREDICTION:\")\n",
        "print(f\"   Best Model: {best_prem_model_name}\")\n",
        "print(f\"   Test R²: {comparison_prem.loc[comparison_prem['Model'] == best_prem_model_name, 'Test_R2'].values[0]:.4f}\")\n",
        "print(f\"   Test RMSE: {comparison_prem.loc[comparison_prem['Model'] == best_prem_model_name, 'Test_RMSE'].values[0]:.2f} ZAR\")\n",
        "print(f\"\\n   Top 5 Features:\")\n",
        "for idx, row in importance_prem.head(5).iterrows():\n",
        "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Business Recommendations will be documented in the final report.\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
